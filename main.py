import os
import re
import asyncio
import json
from datetime import datetime
from difflib import SequenceMatcher

RESUME_PATH = r"Information\\resume_en.md"
COVER_LETTER_PATH = r"Information\\cover_letter.md"
JOB_DESC_PATH = r"Information\\About_job.md"
OUTPUT_DIR = "output"
OPTIMIZED_RESUME = os.path.join(OUTPUT_DIR, "optimized_resume.md")
OPTIMIZED_COVER_LETTER = os.path.join(OUTPUT_DIR, "optimized_cover_letter.md")
COVER_LETTER = os.path.join(OUTPUT_DIR, "cover_letter.md")
ANALYSIS_REPORT = os.path.join(OUTPUT_DIR, "optimization_report.md")
MATRIX_ANALYSIS = os.path.join(OUTPUT_DIR, "compatibility_matrix.json")

# --- Helper Functions ---
def create_directory():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

def read_file(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        print(f"Error reading file: {e}")
        return ""

def save_file(content, file_path):
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        return True
    except Exception as e:
        print(f"Error saving file: {e}")
        return False

import spacy

# Load spaCy model for offline semantic similarity
nlp = spacy.load("en_core_web_sm")

def calculate_similarity(text1, text2):
    """Calculate semantic similarity using spaCy"""
    doc1 = nlp(text1)
    doc2 = nlp(text2)
    return doc1.similarity(doc2)

def extract_key_phrases(text, max_phrases=15):
    """Extract key noun chunks as key phrases using spaCy"""
    doc = nlp(text)
    phrases = [chunk.text.lower() for chunk in doc.noun_chunks]
    freq = {}
    for phrase in phrases:
        freq[phrase] = freq.get(phrase, 0) + 1
    # Sort phrases by frequency and return top max_phrases
    return sorted(freq, key=freq.get, reverse=True)[:max_phrases]

# --- Local Core Analysis Functions ---
def perform_compatibility_analysis(resume, cover_letter, job_desc):
    """Perform multi-layer compatibility analysis between resume, cover letter and job description"""
    resume_keywords = set(extract_key_phrases(resume, max_phrases=50))
    cover_letter_keywords = set(extract_key_phrases(cover_letter, max_phrases=50))
    job_keywords = set(extract_key_phrases(job_desc, max_phrases=50))

    missing_skills = list(job_keywords - (resume_keywords | cover_letter_keywords))
    matched_skills = list((resume_keywords | cover_letter_keywords) & job_keywords)

    skill_gap_score = 100 * len(matched_skills) / len(job_keywords) if job_keywords else 0
    keyword_coverage_score = skill_gap_score  # Simplified for demo

    analysis = {
        "missing_skills": missing_skills,
        "matched_skills": matched_skills,
        "skill_gap_score": skill_gap_score,
        "keyword_coverage_score": keyword_coverage_score,
        "job_desc_keywords": list(job_keywords),
        "addressed_gaps": [],
        "added_keywords": [],
        "quantified_achievements": 0,
        "cover_letter_strategies": "Focus on matched skills and address missing skills.",
        "primary_gaps": missing_skills[:3],
        "strengths": matched_skills[:3],
        "recommended_verbs": ["lead", "develop", "deliver", "manage", "improve"],
        "quantification_examples": ["increased sales by 20%", "reduced costs by 15%"]
    }
    return analysis

def optimize_resume_with_target(resume, analysis):
    """Optimize resume aiming for 10-20% better job match"""
    optimized = resume
    # Add missing keywords naturally at the end of resume for demo purposes
    if analysis["missing_skills"]:
        additions = "\n\n# Added Skills\n" + ", ".join(analysis["missing_skills"][:10])
        optimized += additions
        analysis["added_keywords"] = analysis["missing_skills"][:10]
    # Simulate quantifying achievements
    analysis["quantified_achievements"] = 2
    analysis["addressed_gaps"] = analysis["missing_skills"][:5]
    return optimized

def optimize_cover_letter_with_target(cover_letter, analysis):
    """Optimize cover letter aiming for 10-20% better job match"""
    optimized = cover_letter
    if analysis["missing_skills"]:
        additions = "\n\n# Added Keywords\n" + ", ".join(analysis["missing_skills"][:10])
        optimized += additions
        analysis.setdefault("added_keywords_cover_letter", []).extend(analysis["missing_skills"][:10])
    return optimized

def generate_high_impact_cover_letter(resume, job_desc, analysis):
    """Generate a basic cover letter content"""
    cover_letter = f"""
Dear Hiring Manager,

I am excited to apply for the position. My skills in {', '.join(analysis['matched_skills'][:5])} align well with the job requirements.

I have addressed key gaps such as {', '.join(analysis['missing_skills'][:3])} and am confident in my ability to contribute effectively.

Thank you for considering my application.

Sincerely,
[Your Name]
"""
    return cover_letter.strip()

def calculate_cover_letter_match(cover_letter, job_desc):
    """Calculate how much the cover letter improves job match"""
    cover_keywords = set(extract_key_phrases(cover_letter, max_phrases=50))
    job_keywords = set(extract_key_phrases(job_desc, max_phrases=50))
    matched = cover_keywords & job_keywords
    return len(matched) / len(job_keywords) if job_keywords else 0

def calculate_acceptance_probability(resume, cover_letter, job_desc, analysis):
    """Estimate acceptance probability based on detailed text similarity and keyword coverage"""

    # Calculate initial similarity scores for base probability
    resume_similarity = calculate_similarity(resume, job_desc) if resume else 0
    cover_letter_similarity = calculate_similarity(cover_letter, job_desc) if cover_letter else 0

    # Base probability derived from average similarity scaled to 100%
    base_prob = (resume_similarity + cover_letter_similarity) / 2 * 100

    # Combine resume and cover letter similarity with diminishing returns
    combined_similarity = resume_similarity + cover_letter_similarity - (resume_similarity * cover_letter_similarity)

    # Weighted sum of factors with adjusted weights
    prob = base_prob
    prob += combined_similarity * 40  # combined similarity weight
    prob += analysis.get("skill_gap_score", 0) * 0.2  # skill gap score weight
    prob += len(analysis.get("added_keywords", [])) * 1.0  # added keywords weight

    prob = min(100, prob)
    confidence = 80 + (prob - base_prob) * 0.2
    return {"probability": round(prob), "confidence": round(confidence)}

# --- Reporting ---
def generate_improvement_report(original_resume, optimized_resume, analysis, prob_results):
    """Generate detailed improvement report including cover letter and combined metrics"""
    date_str = datetime.now().strftime("%Y-%m-%d %H:%M")

    # Calculate resume match improvements
    original_resume_sim = calculate_similarity(original_resume, " ".join(analysis["job_desc_keywords"]))
    optimized_resume_sim = calculate_similarity(optimized_resume, " ".join(analysis["job_desc_keywords"]))
    resume_match_improvement = round((optimized_resume_sim - original_resume_sim) * 100, 1)

    # Calculate cover letter similarity improvements
    original_cover_letter = read_file(COVER_LETTER_PATH)
    optimized_cover_letter = read_file(OPTIMIZED_COVER_LETTER)
    original_cover_sim = calculate_similarity(original_cover_letter, " ".join(analysis["job_desc_keywords"]))
    optimized_cover_sim = calculate_similarity(optimized_cover_letter, " ".join(analysis["job_desc_keywords"]))
    cover_letter_match_improvement = round((optimized_cover_sim - original_cover_sim) * 100, 1)

    # Calculate combined resume + cover letter similarity improvements
    combined_original = original_resume + "\n" + original_cover_letter
    combined_optimized = optimized_resume + "\n" + optimized_cover_letter
    original_combined_sim = calculate_similarity(combined_original, " ".join(analysis["job_desc_keywords"]))
    optimized_combined_sim = calculate_similarity(combined_optimized, " ".join(analysis["job_desc_keywords"]))
    combined_match_improvement = round((optimized_combined_sim - original_combined_sim) * 100, 1)

    # Fix combined improvement to be sum of individual improvements or recalc
    combined_match_improvement = resume_match_improvement + cover_letter_match_improvement

    # Calculate acceptance probabilities separately
    original_prob_resume = calculate_acceptance_probability(original_resume, "", "", analysis)
    optimized_prob_resume = calculate_acceptance_probability(optimized_resume, "", "", analysis)

    original_prob_cover = calculate_acceptance_probability("", original_cover_letter, "", analysis)
    optimized_prob_cover = calculate_acceptance_probability("", optimized_cover_letter, "", analysis)

    original_prob_combined = calculate_acceptance_probability(original_resume, original_cover_letter, "", analysis)
    optimized_prob_combined = calculate_acceptance_probability(optimized_resume, optimized_cover_letter, "", analysis)

    prob_improvement_combined = prob_results["final"]["probability"] - prob_results["original"]["probability"]

    # Define some common standards/benchmarks for context
    standards = {
        "target_match_improvement": "10-20%",
        "target_probability_boost": "10-20%",
        "acceptable_confidence_level": "80%+",
        "typical_resume_match": "20-40%",
        "typical_cover_letter_match": "15-30%",
        "typical_acceptance_probability": "50-80%"
    }

    # Fix: Use optimized values for "After" column as Before + Improvement
    after_resume_sim = original_resume_sim + (resume_match_improvement / 100)
    after_cover_sim = original_cover_sim + (cover_letter_match_improvement / 100)
    after_combined_sim = original_combined_sim + (combined_match_improvement / 100)
    after_prob_resume = original_prob_resume['probability'] + (optimized_prob_resume['probability'] - original_prob_resume['probability'])
    after_prob_cover = original_prob_cover['probability'] + (optimized_prob_cover['probability'] - original_prob_cover['probability'])
    after_prob_combined = original_prob_combined['probability'] + prob_improvement_combined
    after_confidence = prob_results['original']['confidence'] + (prob_results['final']['confidence'] - prob_results['original']['confidence'])

    # Read compatibility matrix JSON for report inclusion
    try:
        with open(MATRIX_ANALYSIS, 'r', encoding='utf-8') as f:
            compatibility_matrix = json.load(f)
    except Exception as e:
        compatibility_matrix = None

    # Format compatibility matrix as markdown table if available
    matrix_md = ""
    if compatibility_matrix:
        matrix_md += "\n## Compatibility Matrix\n\n"
        matrix_md += "| Key | Value |\n"
        matrix_md += "|-----|-------|\n"
        for key, value in compatibility_matrix.items():
            matrix_md += f"| {key} | {value} |\n"

    return f"""
# PRECISION OPTIMIZATION REPORT
**Generated**: {date_str}

## Key Metrics
| Metric | Before | After | Improvement | Typical Range |
|--------|--------|-------|-------------|---------------|
| Resume Match | {original_resume_sim:.1%} | {after_resume_sim:.1%} | +{resume_match_improvement}% | {standards['typical_resume_match']} |
| Cover Letter Match | {original_cover_sim:.1%} | {after_cover_sim:.1%} | +{cover_letter_match_improvement}% | {standards['typical_cover_letter_match']} |
| Combined Resume + Cover Letter Match | {original_combined_sim:.1%} | {after_combined_sim:.1%} | +{combined_match_improvement}% | {standards['typical_resume_match']} |
| Acceptance Probability (Resume) | {original_prob_resume['probability']}% | {after_prob_resume}% | +{after_prob_resume - original_prob_resume['probability']}% | {standards['typical_acceptance_probability']} |
| Acceptance Probability (Cover Letter) | {original_prob_cover['probability']}% | {after_prob_cover}% | +{after_prob_cover - original_prob_cover['probability']}% | {standards['typical_acceptance_probability']} |
| Acceptance Probability (Combined) | {original_prob_combined['probability']}% | {after_prob_combined}% | +{prob_improvement_combined}% | {standards['typical_acceptance_probability']} |
| Confidence Level | {prob_results['original']['confidence']}% | {after_confidence}% | +{after_confidence - prob_results['original']['confidence']}% | {standards['acceptable_confidence_level']} |

{matrix_md}


## Optimization Focus Areas
1. **Skill Gaps Addressed:** {len(analysis.get('addressed_gaps', []))}/{len(analysis.get('missing_skills', []))}
2. **Keywords Added:** {len(analysis.get('added_keywords', []))}
3. **Achievements Quantified:** {analysis.get('quantified_achievements', 0)}

## Cover Letter Impact Strategies
{analysis.get('cover_letter_strategies', 'N/A')}

## Standards and Benchmarks
- Target Match Improvement: {standards['target_match_improvement']}
- Target Probability Boost: {standards['target_probability_boost']}
- Acceptable Confidence Level: {standards['acceptable_confidence_level']}

## Improvement Validation
{"âœ… Achieved target match improvement" if 10 <= resume_match_improvement <= 20 else 
 "âš ï¸ Partial improvement - review needed"}

{"âœ… Achieved target probability boost" if 10 <= prob_improvement_combined <= 20 else 
 "âš ï¸ Probability boost below target"}
"""

# --- Main Workflow ---
async def main():
    print("=== Precision Resume Optimization System ===")
    print("Target: 10-20% Match & Probability Improvement")
    create_directory()

    original_resume = read_file(RESUME_PATH)
    original_cover_letter = read_file(COVER_LETTER_PATH)
    job_desc = read_file(JOB_DESC_PATH)

    if not original_resume or not job_desc or not original_cover_letter:
        print("Error: Missing input files")
        return

    print("\nðŸ” Running compatibility analysis...")
    compatibility = perform_compatibility_analysis(original_resume, original_cover_letter, job_desc)
    save_file(json.dumps(compatibility, indent=2), MATRIX_ANALYSIS)

    print("ðŸ“Š Calculating initial probability...")
    original_prob = calculate_acceptance_probability(original_resume, original_cover_letter, job_desc, compatibility)

    print("\nâš™ï¸ Optimizing resume (target: 10-20% improvement)...")
    optimized_resume = optimize_resume_with_target(original_resume, compatibility)
    save_file(optimized_resume, OPTIMIZED_RESUME)

    print("ðŸ“Š Calculating optimized probability...")
    optimized_prob = calculate_acceptance_probability(optimized_resume, original_cover_letter, job_desc, compatibility)

    print("\nâœï¸ Optimizing cover letter (target: 10-20% improvement)...")
    optimized_cover_letter = optimize_cover_letter_with_target(original_cover_letter, compatibility)
    save_file(optimized_cover_letter, OPTIMIZED_COVER_LETTER)

    print("ðŸ“Š Calculating final probability...")
    final_prob = calculate_acceptance_probability(optimized_resume, optimized_cover_letter, job_desc, compatibility)

    print("\nðŸ“ˆ Generating improvement report...")
    prob_results = {
        "original": original_prob,
        "optimized": optimized_prob,
        "final": final_prob
    }

    report = generate_improvement_report(
        original_resume,
        optimized_resume,
        compatibility,
        prob_results
    )
    save_file(report, ANALYSIS_REPORT)

    match_improv = calculate_similarity(optimized_resume, job_desc) - calculate_similarity(original_resume, job_desc)
    cover_letter_improv = calculate_cover_letter_match(optimized_cover_letter, job_desc) - calculate_cover_letter_match(original_cover_letter, job_desc)
    prob_improv = final_prob["probability"] - original_prob["probability"]

    print("\n" + "=" * 60)
    print("ðŸ’Ž OPTIMIZATION RESULTS ðŸ’Ž")
    print("=" * 60)
    print(f"Resume Match Improvement: +{match_improv*100:.1f}%")
    print(f"Cover Letter Match Improvement: +{cover_letter_improv*100:.1f}%")
    print(f"Acceptance Probability Boost: +{prob_improv}%")
    print(f"Report: {ANALYSIS_REPORT}")
    print("\nOptimization completed successfully!")

if __name__ == "__main__":
    asyncio.run(main())
