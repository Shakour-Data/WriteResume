# MCP AI Server with LLaMA Integration

## Setup Instructions

1. Ensure you have Node.js and npm installed.

2. Install dependencies:

```bash
cd mcp-ai-server
npm install
```

3. Install LLaMA inference dependencies:

- You need to have a LLaMA inference engine such as [llama.cpp](https://github.com/ggerganov/llama.cpp) built and accessible in your system PATH or specify the path in the code.

- Download the LLaMA model files (e.g., 7B model) and place them in the `models/llama-7b/` directory as expected by the code.

4. Build the MCP server:

```bash
npm run build
```

5. Run the MCP server:

```bash
npm start
```

## Usage

- The MCP server exposes a tool named `llama_ai_completion` which accepts a prompt string and returns a completion generated by the LLaMA model.

- The server communicates over stdio using the Model Context Protocol.

## Notes

- The current integration uses a placeholder spawn process to run llama.cpp. You may need to adjust the command and arguments in `llama-integration.ts` to match your environment.

- Ensure your system has sufficient CPU resources for inference.

- For further customization or troubleshooting, refer to the llama.cpp documentation.
